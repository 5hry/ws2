[
{
	"uri": "//localhost:1313/vi/",
	"title": "Web application using Terraform",
	"tags": [],
	"description": "",
	"content": "Triển khai ứng dụng web có tính sẵn sàng cao sử dụng Terraform Tổng quan Trong bài lab này, bạn sẽ tìm hiểu các khái niệm cơ bản và thực hành về Amazon System Manager - Session Management. Thực hành tạo kết nối đến máy chủ public và máy chủ private trong VPC.\nNội dung Giới thiệu Các bước chuẩn bị Tạo kết nối đến máy chủ EC2 Quản lý session logs Port Forwarding Dọn dẹp tài nguyên "
},
{
	"uri": "//localhost:1313/vi/1-introduction/",
	"title": "Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Triển khai ứng dụng web có tính sẵn sàng cao bằng Terraform Trong workshop này, chúng ta sẽ xây dựng một ứng dụng web có tính sẵn sàng cao bằng Terraform trên AWS Cloud, chúng ta sẽ dùng Terraform để provision resource chạy ứng dụng web. Tạo AutoScaling Group gồm các EC2 instance chạy ứng dụng web trong kết nối đến database RDS instance. Để đảm bảo tính sẵn sàng cao thì ứng dụng sẽ được deploy trên 2 zones, và có khả năng cân bằng tải cùng với tự động scale khi cần thiết traffic tăng cao, cần xử lí lớn. Để đảm bảo tính bảo mật của ứng dụng ta sẽ đặt EC2 và RDS instance nằm trong các private subnets và có các security groups riêng để người dùng không thể truy cập trực tiếp vào được các EC2 instance được.\nNội dung IaC(Infrastructure as Code) là gì? Terraform basics "
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.1-iac/",
	"title": "IaC là gì?",
	"tags": [],
	"description": "",
	"content": "Thông thường, hầu hết người mới bắt đầu với Cloud nói chung và AWS nói riêng thì đều tiếp xúc với các dịch vụ của AWS qua console, đăng nhập vào console và tương tác với các dịch vụ mình cần. Việc này sẽ khá là nhẹ nhàng thoải mái nếu như chúng ta chỉ tạo một vài ứng dụng nhỏ, không cần phải sử dụng lại cấu hình đó.\nTa có thể hiểu đơn giản theo tên của nó thì ta sẽ viết code để mô tả và chạy những infrastructure của chúng ta. Infrastructure as Code (IaC) giúp chúng ta tương tác với các dịch vụ đó thông qua những dòng code chứ không cần phải lên console để thao tác hoàn toàn bằng tay nữa.\nƯu điểm:\nCó thể tái sử dụng để tạo những ứng dụng khác có nhiều điểm giống nhau để giảm thời gian tạo infra. Có cấu trúc rõ ràng, lưu lại trạng của các infra được tạo ra. Bạn hãy thử tưởng tượng việc một hệ thống lớn mà bạn tạo ra infra rồi nhưng qua hôm sau lại quên mất là mình đã tạo chưa. Có thể backup lại hạ tầng khi có sự cố xảy ra với hệ thống. "
},
{
	"uri": "//localhost:1313/vi/3-vpc/3.1-vpc-main/",
	"title": "Tạo VPC",
	"tags": [],
	"description": "",
	"content": "Để tạo một VPC như kiến trúc high availability ở đầu Workshop, ta cần tạo:\n1 VPC. 2 public subnets. 2 private subnets cho EC2 chạy ứng dụng web. 2 private subnets cho RDS instance chạy database. 1 NAT Gateway cho các EC2 có thể truy cập ra ngoài internet. 1 Internet Gateway nhận traffic từ internet vào VPC. VPC Trước tiên, ta tạo một thư mục vpc trong thư mục modules, tạo file main.tf sau đó vào file main.tf gốc để khai báo module VPC này bằng cách thêm đoạn code như bên dưới để khai báo module vpc nằm trong thư mục ./modules/vpc .\nmodule \u0026#34;vpc\u0026#34; {\rsource = \u0026#34;./modules/vpc\u0026#34;\r} Sau khi thêm đoạn code này vào file main.tf gốc thì ta cần chạy lại lệnh terraform init để Terraform nhận biết được là chúng ta đang chạy những file .tf ở thư mục khác và tải về các thư viện cần thiết nếu cần.\nĐể tạo một VPC, ta chỉ cần khai báo \u0026ldquo;resource aws_vpc\u0026rdquo;, sau đó thêm vào các thuộc tính của VPC.\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; {\rcidr_block = \u0026#34;10.0.0.0/16\u0026#34;\rtags = {\rName = \u0026#34;${var.app_name} VPC\u0026#34;\r}\r} Ở đây, ta tạo một VPC có Name là main với CIDR block \u0026ldquo;10.0.0.0/16\u0026rdquo; và đánh tag Name là \u0026quot;${var.app_name} VPC\u0026quot; . ${var.app_name} là lấy giá trị biến app_name được khai báo trong file variables.tf cùng cấp. Ví dụ biến app_name ở dưới là dạng chuỗi và mặc định giá trị là \u0026ldquo;Workshop2\u0026rdquo; thì Name của VPC được tạo ra sẽ là \u0026ldquo;Workshop2 VPC\u0026rdquo;.\nvariable \u0026#34;app_name\u0026#34; {\rtype = string\rdefault = \u0026#34;Workshop2\u0026#34;\r} Vậy là ta đã khai báo được một VPC với CIDR block là \u0026ldquo;10.0.0.0/16\u0026rdquo;\nPublic Subnets Sau khi đã có VPC, ta có thể tạo các resource nằm bên trong VPC này. Đầu tiên ta sẽ tạo 2 Public subnets để chứa NAT gateway cho EC2 trong private subnets có thể truy cập ra Internet. Để tạo các subnets, ta khai báo resource aws_subnet trong file main.tf. Ở đây ta cần tạo 2 subnets ở 2 zones, thay vì tạo từng cái thì ta có thể thêm thuộc tính count bằng độ dài của biến public_subnet_cidrs khai báo dải CIDR của các public subnets bằng 2 để tạo 1 lần 2 subnets. vpc_id sẽ gán bằng id của VPC vừa được tạo. Ta có thể lấy giá trị bằng cách gọi resource_type.resource_name.feature, ví dụ như aws_vpc.main.id. CIDR block thì ta có thể dùng hàm element để lấy ra giá trị cidr block nằm trong biến public_subnet_cidrs thông qua count.index để xác định thứ tự cần lấy. Tương tự với AZ, mỗi subnet sẽ ở một availability zone tương ứng với giá trị count.index. Vậy theo đoạn code bên dưới thì sẽ tạo ra 1 subnet với CIDR block là \u0026ldquo;10.0.1.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1a\u0026rdquo; với Name \u0026ldquo;Workshop2 Public Subnets 1\u0026rdquo; và 1 subnet với CIDR block là \u0026ldquo;10.0.2.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1b\u0026rdquo; với Name \u0026ldquo;Workshop2 Public Subnets 2\u0026rdquo;\nresource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnets\u0026#34; {\rcount = length(var.public_subnet_cidrs)\rvpc_id = aws_vpc.main.id\rcidr_block = element(var.public_subnet_cidrs, count.index)\ravailability_zone = element(var.azs, count.index)\rtags = {\rName = \u0026#34;${var.app_name} Public Subnets ${count.index + 1}\u0026#34;\r}\r} variable \u0026#34;public_subnet_cidrs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Public Subnet CIDR values\u0026#34;\rdefault = [\u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;10.0.2.0/24\u0026#34;]\r}\rvariable \u0026#34;azs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Availability Zones Name\u0026#34;\rdefault = [\u0026#34;ap-southeast-1a\u0026#34;, \u0026#34;ap-southeast-1b\u0026#34;]\r} Private Web Subnets Bước tiếp theo, ta tạo 2 private subnets để chứa các EC2 chạy ứng web. Tương tự với tạo public subnets, thì ta sẽ tạo ra được 2 private subnets, 1 subnet với CIDR block là \u0026ldquo;10.0.3.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1a\u0026rdquo; với Name \u0026ldquo;Workshop2 Private Subnets 1\u0026rdquo; và 1 subnet CIDR block là \u0026ldquo;10.0.4.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1b\u0026rdquo; với Name \u0026ldquo;Workshop2 Private Subnets 2\u0026rdquo;\nresource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnets\u0026#34; {\rcount = length(var.private_subnet_cidrs)\rvpc_id = aws_vpc.main.id\rcidr_block = element(var.private_subnet_cidrs, count.index)\ravailability_zone = element(var.azs, count.index)\rtags = {\rName = \u0026#34;${var.app_name} Private Subnets ${count.index + 1}\u0026#34;\r}\r} variable \u0026#34;private_subnet_cidrs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Private Subnet CIDR values\u0026#34;\rdefault = [\u0026#34;10.0.3.0/24\u0026#34;, \u0026#34;10.0.4.0/24\u0026#34;]\r}\rvariable \u0026#34;azs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Availability Zones Name\u0026#34;\rdefault = [\u0026#34;ap-southeast-1a\u0026#34;, \u0026#34;ap-southeast-1b\u0026#34;]\r} Private Database Subnets Tương tự, ta tạo 2 private subnets để chứa 2 RDS instances. Tương tự với tạo ở trên, thì ta sẽ tạo ra được 2 private subnets, 1 subnet với CIDR block là \u0026ldquo;10.0.5.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1a\u0026rdquo; với Name \u0026ldquo;Workshop2 Database Subnets 1\u0026rdquo; và 1 subnet CIDR block là \u0026ldquo;10.0.6.0/24\u0026rdquo; ở zone \u0026ldquo;ap-southeast-1b\u0026rdquo; với Name \u0026ldquo;Workshop2 Database Subnets 2\u0026rdquo;\nresource \u0026#34;aws_subnet\u0026#34; \u0026#34;database_subnets\u0026#34; {\rcount = length(var.database_subnet_cidrs)\rvpc_id = aws_vpc.main.id\rcidr_block = element(var.database_subnet_cidrs, count.index)\ravailability_zone = element(var.azs, count.index)\rtags = {\rName = \u0026#34;${var.app_name} Database Subnets ${count.index + 1}\u0026#34;\r}\r} variable \u0026#34;database_subnet_cidrs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Database Subnet CIDR values\u0026#34;\rdefault = [\u0026#34;10.0.5.0/24\u0026#34;, \u0026#34;10.0.6.0/24\u0026#34;]\r}\rvariable \u0026#34;azs\u0026#34; {\rtype = list(string)\rdescription = \u0026#34;Availability Zones Name\u0026#34;\rdefault = [\u0026#34;ap-southeast-1a\u0026#34;, \u0026#34;ap-southeast-1b\u0026#34;]\r} Internet Gateway Sau khi ta đã tạo các subnets thì traffic vẫn chưa được route, cần tạo Internet Gateway để traffic có thể ra vào VPC. Ta tạo một resource aws_internet_gateway nằm trong vpc trên, được đánh tag Name là \u0026ldquo;Workshop2 Internet Gateway\u0026rdquo; để route traffic ra vào VPC.\nresource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;igw\u0026#34; {\rvpc_id = aws_vpc.main.id\rtags = {\rName = \u0026#34;${var.app_name} Internet Gateway\u0026#34;\r}\r} Sau khi đã có Internet Gateway, ta cần cấu hình route table để có thể route traffic từ public subnet có thể ra vào VPC thông qua Internet Gateway. Ta tạo resource aws_route_table trong vpc main route traffic đến dải CIDR \u0026ldquo;0.0.0.0/0\u0026rdquo; đi đến Internet Gateway, và ta sẽ gán 2 public subnets vào route table này dể traffic từ public subnet sẽ được route đến Internet Gateway. Ta tạo thêm resource là aws_route_table_association để attach 2 public subnets vào route table đó.\nresource \u0026#34;aws_route_table\u0026#34; \u0026#34;second_rt\u0026#34; {\rvpc_id = aws_vpc.main.id\rroute {\rcidr_block = \u0026#34;0.0.0.0/0\u0026#34;\rgateway_id = aws_internet_gateway.igw.id\r}\rtags = {\rName = \u0026#34;${var.app_name} 2nd Route Table\u0026#34;\r}\r}\rresource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_subnets_association\u0026#34; {\rcount = length(var.public_subnet_cidrs)\rroute_table_id = aws_route_table.second_rt.id\rsubnet_id = element(aws_subnet.public_subnets[*].id, count.index)\r} NAT Gateway Sau khi tạo route table và gán public subnets vào để traffic từ public subnets ra ngoài internet có thể đi qua Internet Gateway, thì nếu EC2 trong Private subnet muốn pull image hay cập nhật bản vá thì cần ra ngoài internet nhưng không thể facing ra internet, vì vậy ta sẽ tạo NAT gateway để EC2 trong Private subnet có thể ra ngoài internet được. Muốn tạo được NAT gateway thì ta cần phải cấp cho nó một public IP, ta sẽ tạo một Elastic IP để cấp cho NAT gateway này. Tạo một resource aws_eip với domain = \u0026ldquo;vpc\u0026rdquo; để cấp phát cho NAT trong VPC. Sau đó, khi traffic route đến NAT thì NAT sẽ route đến Internet Gateway để đi ra Internet vì NAT nằm ở public subnets và được cấu hình để ra Internet thông qua Internet Gateway.\nresource \u0026#34;aws_eip\u0026#34; \u0026#34;nat_gw_eip\u0026#34; {\rdomain = \u0026#34;vpc\u0026#34;\rtags = {\rName = \u0026#34;${var.app_name} EIP NAT\u0026#34;\r}\r} Sau khi có được Elastic IP, ta tạo NAT Gateway nằm trong một public subnet đã tạo và được gán cho IP là Elastic IP vừa tạo.\nresource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;nat_gw\u0026#34; {\rallocation_id = aws_eip.nat_gw_eip.id\rsubnet_id = aws_subnet.public_subnets[0].id\rtags = {\rName = \u0026#34;${var.app_name} NAT Gateway\u0026#34;\r}\r} Tương tự khi tạo Internet Gateway, ta cần chỉ cho private subnets biết được đường đi ra ngoài Internet bằng cách cấu hình route table route traffic từ Private subnets tới NAT Gateway, đoạn code dưới cũng tương tự đoạn code tạo route table cho Internet Gateway chỉ khác là đổi từ Public subnets thành Private subnet và đổi từ Internet Gateway ID thành NAT Gateway ID.\nresource \u0026#34;aws_route_table\u0026#34; \u0026#34;nat_gw_rt\u0026#34; {\rvpc_id = aws_vpc.main.id\rroute {\rcidr_block = \u0026#34;0.0.0.0/0\u0026#34;\rgateway_id = aws_nat_gateway.nat_gw.id\r}\rtags = {\rName = \u0026#34;${var.app_name} NAT Route Table\u0026#34;\r}\r}\rresource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_subnet_asso_natgw\u0026#34; {\rcount = length(var.private_subnet_cidrs)\rroute_table_id = aws_route_table.nat_gw_rt.id\rsubnet_id = element(aws_subnet.private_subnets[*].id, count.index)\r} Vậy là sau viết những đoạn code để tạo một VPC để chứa web application thì chúng ta sẽ chạy thử để xem kết quả. Mở Terminal chạy terraform plan để xem những resources nào sẽ được tạo và terraform apply và điền yes để xác nhận tạo những resources đó.\nVà kết quả là ta đã tạo được một VPC để chứa các tài nguyên, traffic từ private subnets được route đến NAT gateway và traffic từ public subnets được route đến Internet Gateway trong khi database subnet không cần route traffic vì các RDS là Managed Service nên được AWS quản lý hoàn toàn.\nSau khi đã kiểm tra tất cả những resources được tạo ra đã chính xác với mong muốn thì ta có thể dùng lệnh terraform destroy để xóa toàn bộ những resources đó để tiết kiệm $$$ rồi tiếp tục viết code để tạo thêm những modules khác.\n"
},
{
	"uri": "//localhost:1313/vi/2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "\rBạn cần có sẵn một ứng dụng web, nếu như không có thì không thể làm tiếp được. Có thể tham khảo và sử dụng ứng dụng web ở đây\nNgoài ra, nên có một domain name và tạo trước một hosted zone để tạo những tạo các CNAME record để set các URL của của database, application.\nBạn có thể mua domain ở Route53 hoặc mua của bên thứ 3 rồi add vào Route53.\nBây giờ ta có thể follow theo các bước trong Workshop mà không cần lo lắng thiếu gì nữa.\nTrong các phần tiếp theo, chúng ta sẽ lần lượt tạo các modules Terraform để provision resources nằm trong các modules đó.\n"
},
{
	"uri": "//localhost:1313/vi/1-introduction/1.2-terraform/",
	"title": "Terraform là gì?",
	"tags": [],
	"description": "",
	"content": "Hiện tại thì Terraform có lẽ là tool IaC thông dụng nhất ở thời điểm hiện tại. Nó là open-source của HashiCorp chuyên dùng để tạo ra infrastructure thông qua những dòng code mà chúng ta viết thay vì chúng ta phải lên console để tạo tốn khá nhiều thời gian.\nCó tool khác cũng có thể làm được như này, như là Ansible, tuy nhiên Ansible lại là một Configuration Management tool chứ không tập trung vào IaC như Terraform nên việc sử dụng Ansible sẽ cần phải chạy thêm những thứ không cần thiết.\nƯu điểm:\nOpen-source, miễn phí. Cộng đồng lớn. Declarative programing. Một ưu điểm khá là lớn: có thể cung cấp hạ tầng cho nhiều Cloud Provider(AWS, GCP, Azure) trong cùng một file cấu hình. Ví dụ sử dụng Terraform: Trước khi đi vào triển khai Terraform, ta cần cài đặt AWS CLI và Terraform CLI\nCài AWS CLI. AWS CLI Configuration Cài Terraform CLI. Terraform CLI Configuration Để triển khai hạ tầng bằng Terraform, ta cần biết các bước, cũng như một số câu lệnh thông dụng khi sử dụng Terraform:\nĐầu tiên, ta cần tạo một workspace, có thể là một thử mục ở máy local.\nTiếp theo, viết code khai báo các hạ tầng mình muốn triển khai. Ta có thể khai báo các resources theo cấu trúc sau. Ở dưới, ta sử dụng một block tên là resources, đây là block quan trọng nhất trong terraform, chúng ta sẽ sử dụng block này để tạo resources. Type là khai báo loại resource mà ta muốn tạo và cuối cùng là Name là tên mà mình đặt cho resource đó =\u0026gt; Phục vụ cho việc truy xuất thông tin. .\nNhư hình ở dưới thì đầu tiên ta cần khai báo provider, ở đây là \u0026ldquo;aws\u0026rdquo;, tạo thêm một EC2 instance loại \u0026ldquo;t2.micro\u0026rdquo; có name là \u0026ldquo;My EC2 Instance\u0026rdquo; ở region Singapore.\nĐể khởi tạo workspace, ta chạy câu lệnh terraform init để tải aws provider xuống workspace hiện tại để Terraform có thể sử dụng những provider này và gọi API để khởi tạo resource cho chúng ta. Sau khi chạy xong thì thư mục .terraform xuất hiện trong workspace và chứa code của các providers mà ta đã khai báo.\nỞ đây ta có thể chạy lệnh terraform fmt để chỉnh lại format của các file terraform.\nNgoài ra để kiểm tra xem tính hợp lệ của những đoạn code mà ta đã viết, ta có thể dùng lệnh terraform validate\nSau khi khởi tạo xong workspace bằng terraform init, ta chạy câu lệnh terraform plan để xem thông những resource nào sẽ được tạo/sửa/xóa. ⇒ Tạo thêm 1 resource, 0 sửa, 0 xóa. Sau khi kiểm tra plan thì ta chạy lệnh terraform apply để thực hiện những hành động đó.\nHạn chế sửa/xóa resource được tạo bằng Terraform vì sẽ làm mất đi tính trạng thái của resource được quản lý bởi Terraform.\nNếu trong trường hợp đã apply và khởi tạo resource nhưng có sai sót và muốn sửa thì có thể sửa trực tiếp trong code và chạy lệnh terraform apply một lần nữa để Terraform xác định sửa đổi để sửa thông tin mà không cần phải xóa resource chạy lại.\nSau khi chạy terraform plan nếu sửa code lại thì lệnh apply vẫn sẽ chạy code mới, nếu muốn vẫn chạy code lúc chạy lệnh terraform plan thì thêm “-out=\u0026lt;filename\u0026gt;”. Sau đó chạy terraform apply \u0026lt;filename\u0026gt;.\nSau khi chạy xong lệnh apply thì EC2 Instance với tên \u0026ldquo;My EC2 Instance\u0026rdquo; có type \u0026ldquo;t2.micro\u0026rdquo; đã được tạo. Cuối cùng nếu muốn xóa resource thì chỉ cần chạy lệnh “terraform destroy”. Terraform sẽ xóa toàn bộ những resource đã được triển khai từ đầu.\n⇒ Tóm lại, Terraform có thể được hiểu đơn giản là một công cụ quản lý resource state thông qua file terraform.tfstate được tạo khi chạy lệnh terraform apply. Và nó còn giúp thực hiện các hành động CRUD lên các resource của infra mình muốn.\n"
},
{
	"uri": "//localhost:1313/vi/3-vpc/3.2-vpc-output/",
	"title": "VPC Output cho các module khác",
	"tags": [],
	"description": "",
	"content": "Sau khi tạo xong module VPC, ta sẽ tiếp tục tạo các modules khác. Tuy nhiên, giả sự như ta tạo một EC2, ta cần biết được private subnet, \u0026hellip; những thông tin từ module VPC, vì vậy ta cần phải cho các modules khác biết được những thông tin đó thông qua file output.tf.\noutput \u0026#34;vpc_id\u0026#34; {\rvalue = aws_vpc.main.id\r}\routput \u0026#34;private_subnets\u0026#34; {\rvalue = aws_subnet.private_subnets[*].id\r}\routput \u0026#34;database_subnets\u0026#34; {\rvalue = aws_subnet.database_subnets[*].id\r}\routput \u0026#34;public_subnets\u0026#34; {\rvalue = aws_subnet.public_subnets[*].id\r}\routput \u0026#34;internet_gw\u0026#34; {\rvalue = aws_internet_gateway.igw\r} Ở trên ta sẽ output ra vpc_id cho các module như EC2, Load Balancer và Database để tạo những resource liên quan trong VPC này. Ngoài ra ta cũng output ra những subnets id được tạo để phục vụ cho việc tạo Bastion Host, AutoScaling Group, RDS instance,\u0026hellip; Và cuối cùng sẽ output internet_gw để ALB nhận traffic thông qua nó.\n"
},
{
	"uri": "//localhost:1313/vi/3-vpc/",
	"title": "Khởi tạo Workspace",
	"tags": [],
	"description": "",
	"content": "Đầu tiên, ta cần khởi tạo Workspace như ở Introduction ta sẽ tạo một file terraform như \u0026ldquo;main.tf\u0026rdquo; với nội dung như hình để khai báo version và provider.\nterraform {\rrequired_providers {\raws = {\rsource = \u0026#34;hashicorp/aws\u0026#34;\rversion = \u0026#34;~\u0026gt; 5.0\u0026#34;\r}\r}\r}\rprovider \u0026#34;aws\u0026#34; {\rregion = \u0026#34;ap-southeast-1\u0026#34;\r} Tiếp theo, ta sẽ chạy lệnh terraform init để khởi tạo và Terraform tải về máy những thư viện cần thiết.\nVì ở Workshop này sẽ cần tạo khá nhiều resource nên ta cần phải chia ra thành các modules để có thể quản lý các resource dễ hơn. Ta sẽ tạo một thư mục modules cùng cấp với file main.tf vừa tạo để chứa các modules sắp tạo tiếp theo.\nTiếp theo, chúng ta sẽ thực hiện tạo VPC modules, gồm các subnets để chứa các EC2 chạy ứng dụng web và RDS instances để chạy database.\nNội dung 3.1. Tạo VPC 3.2. Output cho các modules khác\n"
},
{
	"uri": "//localhost:1313/vi/4-autoscalinggroup/",
	"title": "Tạo Application Instance",
	"tags": [],
	"description": "",
	"content": "Tiếp theo, chúng ta sẽ thực hiện tạo EC2 module, gồm AutoScaling Group chứa các EC2 instance để chạy ứng dụng web, Security Group và IAM Role để pull image từ ECR về và chạy ứng dụng. Trước tiên ta sẽ khai báo module EC2 trong file main.tf ở thư mục gốc. Có những biến được nhận từ các module khác như vpc_id, private_subnets_id, public_subnets_id, từ module vpc, load_balancer_id(id của ALB)và target_group_arn(target group được tạo để ALB route traffic đến) từ module load_balancer.\nmodule \u0026#34;ec2\u0026#34; {\rsource = \u0026#34;./modules/ec2\u0026#34;\rvpc_id = module.vpc.vpc_id\rprivate_subnets_id = module.vpc.private_subnets\rpublic_subnets_id = module.vpc.public_subnets\rload_balancer_id = module.loadbalancer.load_balancer_id\rtarget_group_arn = module.loadbalancer.target_group_arn\r} Để tạo một AutoScaling Group như kiến trúc ở đầu Workshop, ta cần tạo:\nLaunch Template. IAM Role cho EC2. Security groups cho EC2 trong public và private subnets. AutoScaling group, AutoScaling policies. Bastion Host. ASG = AutoScaling Group\nLaunch Template Ta cần tạo một Launch Template cho AutoScaling Group để khi nó tạo thêm một EC2 thì nó sẽ được tạo theo template đó. Template này sẽ chứa các thuộc tính của EC2 như Type, AMI, Keypair, Security Group,\u0026hellip;\nTrước tiên, ta sẽ tạo 1 role để cấp cho EC2 có quyền đọc ECR để pull image từ ECR và chạy container. Ngoài ra, ta cần cho EC2 này đọc file init.sql được lưu trên S3 để sao đó chạy trong RDS nên cũng cần quyền đọc S3. Sau đó, ta đọc dữ liệu aws_iam_instance_profile và đặt tên web_app_instance_profile, để gán cho EC2 role này. Như ở trên đã nói, tiếp theo ta sẽ tạo một launch template để khai báo những thông tin của EC2 sẽ được tạo như AMI, key name, instance type được khai báo trong file variables.tf và security group được tạo ở dưới mở port 80 chỉ lắng nghe từ public subnets(nơi đặt ALB), \u0026hellip; Ngoài ra còn có user data, nơi chứa những câu lệnh được chạy ngay khi EC2 được launch.\ndata \u0026#34;aws_iam_instance_profile\u0026#34; \u0026#34;web_app_instance_profile\u0026#34; {\rname = \u0026#34;web-app-ec2\u0026#34;\r}\rresource \u0026#34;aws_launch_template\u0026#34; \u0026#34;instances_configuration\u0026#34; {\rname_prefix = \u0026#34;asg-instance\u0026#34;\rimage_id = var.ami_name\rkey_name = var.key_name\rinstance_type = var.instance_type\ruser_data = filebase64(\u0026#34;./install_script.sh\u0026#34;)\rvpc_security_group_ids = [aws_security_group.private_subnet_sg.id]\riam_instance_profile {\rname = data.aws_iam_instance_profile.web_app_instance_profile.name\r}\rlifecycle {\rcreate_before_destroy = true\r}\rtags = {\rName = \u0026#34;asg-instance\u0026#34;\r}\r}\rresource \u0026#34;aws_security_group\u0026#34; \u0026#34;private_subnet_sg\u0026#34; {\rvpc_id = var.vpc_id\rname = \u0026#34;Private Subnet SG\u0026#34;\rdescription = \u0026#34;Allow traffic from Public Subnets\u0026#34;\ringress {\rdescription = \u0026#34;SSH ingress from public subnets\u0026#34;\rfrom_port = 22\rto_port = 22\rprotocol = \u0026#34;tcp\u0026#34;\rsecurity_groups = [aws_security_group.public_subnet_sg.id, ]\r}\ringress {\rfrom_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; security_groups = [aws_security_group.public_subnet_sg.id, ]\r}\ringress {\rdescription = \u0026#34;Http traffic\u0026#34;\rfrom_port = 80\rto_port = 80\rprotocol = \u0026#34;tcp\u0026#34;\rsecurity_groups = [aws_security_group.public_subnet_sg.id, ]\r}\regress {\rdescription = \u0026#34;Allow all traffic to internet\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\rprotocol = \u0026#34;-1\u0026#34;\rfrom_port = 0\rto_port = 0\r}\rtags = {\rName = \u0026#34;Private Subnet SG\u0026#34;\r}\r} Dưới đây là user data cho EC2 khi được khởi động. Cài docker, và đăng nhập để pull image về từ ECR, sau đó chạy container từ image đó và map port 80:80.\n#!/bin/bash\rsudo yum update sudo yum install -y docker\rsudo systemctl enable docker\rsudo service docker start\raws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 905418377779.dkr.ecr.ap-southeast-1.amazonaws.com\rREPOSITORY_URI=905418377779.dkr.ecr.ap-southeast-1.amazonaws.com/ws1-bluegreen-repo\rIMAGE_TAG=\u0026#34;latest\u0026#34;\rdocker pull $REPOSITORY_URI:$IMAGE_TAG\rCONTAINER_NAME=\u0026#34;app-container\u0026#34;\rPORT_MAPPING=\u0026#34;80:80\u0026#34;\rif [ $(docker ps -a -q -f name=$CONTAINER_NAME) ]; then\recho \u0026#34;Stopping and removing existing container: $CONTAINER_NAME\u0026#34;\rdocker stop $CONTAINER_NAME\rdocker rm $CONTAINER_NAME\rfi\rdocker run -d --name $CONTAINER_NAME -p $PORT_MAPPING $REPOSITORY_URI:$IMAGE_TAG\rif [ $(docker ps -q -f name=$CONTAINER_NAME) ]; then\recho \u0026#34;Container is running: $CONTAINER_NAME\u0026#34;\relse\recho \u0026#34;Failed to run container: $CONTAINER_NAME\u0026#34;\rexit 1\rfi AutoScaling Group Sau khi có Launch Template, ta sẽ tạo AutoScaling Group từ launch Template đó. Ở đây, ta tạo một resource aws_autoscaling_group có min_size là 2, max_size là 6 ↔ min mỗi AZ là 1 và max là 3 EC2, health check period là 150s = 2,5m và health bằng ELB, sau đó khai báo zone bằng những private subnets được tạo để chứa EC2, và target group ARN là nhận từ biến được output từ module Load Balancer ⇒ các EC2 nằm trong ASG này sẽ ở trong target group để ALB route traffic đến, và khai báo launch_template được tạo ở trên. Ta có thể tạo tag name cho những EC2 bằng tag key value.\nresource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;asg\u0026#34; {\rname = \u0026#34;asg\u0026#34;\rmin_size = 2\rmax_size = 6\rdesired_capacity = 2\rhealth_check_grace_period = 150\rhealth_check_type = \u0026#34;ELB\u0026#34;\rvpc_zone_identifier = var.private_subnets_id\rtarget_group_arns = [var.target_group_arn]\rlaunch_template {\rid = aws_launch_template.instances_configuration.id\rversion = \u0026#34;$Latest\u0026#34;\r}\rtag {\rkey = \u0026#34;Name\u0026#34;\rvalue = \u0026#34;Web Application\u0026#34;\rpropagate_at_launch = true\r}\r} Và để ASG biết khi nào thì launch thêm EC2, và khi nào thì terminate bớt đi thì ta cần tạo những Autoscaling Policies. Có thể tạo nhiều loại Autoscaling Policies như TargetTrackingScaling dựa trên các metrics như trung bình CPU, trung bình network traffic in và out, và số request trên từng EC2 như hình dưới. Hoặc Policy Simple Scaling sẽ Add/Remove/Set to số lượng EC2 khi có alarm từ CloudWatch hoặc Policy Step Scaling sẽ là nhiều Simple Scaling liên tiếp nhau. Ở workshop này, ASG Policy mình sẽ dùng TargetTrackingScaling để theo dõi metric CPU trung bình khi lớn hơn 60%, mình sẽ tăng thêm 1 EC2.\nresource \u0026#34;aws_autoscaling_policy\u0026#34; \u0026#34;avg_cpu_policy_greater\u0026#34; {\rname = \u0026#34;avg-cpu-policy-greater\u0026#34;\rpolicy_type = \u0026#34;TargetTrackingScaling\u0026#34;\rautoscaling_group_name = aws_autoscaling_group.asg.id\rtarget_tracking_configuration {\rpredefined_metric_specification {\rpredefined_metric_type = \u0026#34;ASGAverageCPUUtilization\u0026#34;\r}\rtarget_value = 60.0\r}\r} Bastion Host Ở đây, mình sẽ dùng Bastion Host thay thì SSM vì mình đặt các EC2 trong private subnets nên việc mở Endpoint cho cả 2 subnets sẽ tốn tới 6 Endpoints khá tốn kém.\nTrước tiên, mình sẽ tạo một SG để có thể SSH đến Bastion Host chỉ bằng IP của nhà mình. Mở port 22 từ địa chỉ my_IP được khai báo trong file variables.tf.\nVì trong Ingress chỉ nhận vào cidr_blocks nên nếu muốn để một địa chỉ IP duy nhất của mình thì ta sẽ nhập your_IP/32 với subnet mask là 32 thì sẽ chỉ có duy nhất địa chỉ IP của bạn khớp với cidr_blocks này.\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;public_subnet_sg\u0026#34; {\rvpc_id = var.vpc_id\rname = \u0026#34;Public Subnet SG\u0026#34;\rdescription = \u0026#34;Allow SSH from my home\u0026#34;\ringress {\rdescription = \u0026#34;SSH ingress\u0026#34;\rfrom_port = 22\rto_port = 22\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [var.my_IP, ]\r}\regress {\rdescription = \u0026#34;Allow all traffic to internet\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\rprotocol = \u0026#34;-1\u0026#34;\rfrom_port = 0\rto_port = 0\r}\rtags = {\rName = \u0026#34;Public Subnet SG\u0026#34;\r}\r} Sau khi tạo SG, ta sẽ tạo EC2 Bastion với các thông số cơ bản, và đặc biệt lưu ý cấp phát Public IP cho nó để có thể truy cập từ ngoài Internet. Nếu muốn ta có thể output ra khi chạy terraform apply để lấy IP và connect đến nhanh hơn thay vì lên console để copy.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;bastion_ec2\u0026#34; {\rami = var.ami_name\rkey_name = var.key_name\rinstance_type = var.instance_type\rvpc_security_group_ids = [aws_security_group.public_subnet_sg.id]\rsubnet_id = var.public_subnets_id[0]\rassociate_public_ip_address = true\r} output \u0026#34;bastion_IP\u0026#34; {\rvalue = aws_instance.bastion_ec2.public_ip\r}\r# Đoạn này được đặt trong file output.tf để phân biệt và tìm kiếm dễ hơn. Vì mình đang ở module EC2 nên output vẫn chưa được in ra khi chạy xong và chỉ được in ra khi nằm trong thư mục gốc. Vậy ta cần tạo lên một output tại thư mục gốc.\noutput \u0026#34;bastion_IP\u0026#34; {\rdescription = \u0026#34;Bastion\u0026#39;s Public IP address\u0026#34;\rvalue = module.ec2.bastion_IP\r} Cuối cùng, ta sẽ tạo một output nữa là private_security_groups để khi tạo Security Group cho RDS instance có thể cho traffic từ SG này đến.\n"
},
{
	"uri": "//localhost:1313/vi/5-loadblancer/",
	"title": "Tạo Load Balancer",
	"tags": [],
	"description": "",
	"content": "Tiếp theo, chúng ta sẽ thực hiện tạo Load Balancer module, gồm ALB, Listener, Target Group để khi ALB lắng nghe trên port đã được cấu hình từ Listener sẽ route traffic đến Target Group. Trước tiên ta sẽ khai báo module Load Balancer trong file main.tf ở thư mục gốc. Có những biến được nhận từ các module khác như vpc_id, internet_gw, public_subnets từ module vpc, certificate_arn từ module route53 để cấu hình HTTPS cho ALB.\nmodule \u0026#34;loadbalancer\u0026#34; {\rsource = \u0026#34;./modules/load_balancer\u0026#34;\rpublic_subnets_id = module.vpc.public_subnets\rvpc_id = module.vpc.vpc_id\rinternet_gw = module.vpc.internet_gw.id\rcertificate_arn = module.route53.cert_arn\r} Để tạo một Load Balancer như trên, ta cần tạo:\n1 ALB 1 Listener 1 Target Group 1 Security Group cho ALB. ALB Ở đây, ta sẽ tạo một Load Balancer với type là Application và facing Internet, được dặt ở 2 public subnets đã tạo trong module vpc và được tạo sau khi Internet Gateway đã hoàn tất vì traffic đi qua Internet Gateway mới tới ALB được, cuối cùng ALB này có securiy group được tạo bên dưới. Security Group này đơn giản nhận traffic từ 2 port 80(HTTP) và 443(HTTPS) từ Internet.\nresource \u0026#34;aws_lb\u0026#34; \u0026#34;my_alb\u0026#34; {\rname = \u0026#34;my-alb\u0026#34;\rinternal = false\rload_balancer_type = \u0026#34;application\u0026#34;\rsecurity_groups = [aws_security_group.alb_sg.id]\rsubnets = var.public_subnets_id\rdepends_on = [var.internet_gw]\rtags = {\rName = \u0026#34;Load Balancer\u0026#34;,\r}\r}\rresource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb_sg\u0026#34; {\rname = \u0026#34;alb-security-group\u0026#34;\rdescription = \u0026#34;Security group for Application Load Balancer\u0026#34;\rvpc_id = var.vpc_id ingress {\rfrom_port = 80\rto_port = 80\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\ringress {\rfrom_port = 443\rto_port = 443\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\regress {\rfrom_port = 0\rto_port = 0\rprotocol = \u0026#34;-1\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\rtags = {\rName = \u0026#34;ALB Security Group\u0026#34;\r}\r} Sau đó ta sẽ tạo một Target Group để traffic route đến những EC2 trong group này. Port khi kết nối đến các EC2 trong group này là sẽ 80 với giao thức HTTP. Vì ở đây đã nằm trong VPC của mình rồi nên có thể giao tiếp bằng HTTP port 80. Target group này chính là AutoScaling Group đã tạo ở module EC2, điều này có nghĩa là traffic sẽ được route đến các EC2 nằm trong ASG đó.\nresource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;web_app_TG\u0026#34; {\rname = \u0026#34;web-app-TG\u0026#34;\rport = 80\rprotocol = \u0026#34;HTTP\u0026#34;\rvpc_id = var.vpc_id\r} Tiếp theo, để ALB route traffic đến Target Group, ta cần một Listener. Ta sẽ tạo một resource aws_lb_listener từ ALB vừa tạo với giao thức HTTPS port 443 và ssl_policy ELBSecurityPolicy-TLS13-1-2-2021-06 (Cái này mình lấy từ trang chủ của AWS)\nresource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;alb_listener_https\u0026#34; {\rload_balancer_arn = aws_lb.my_alb.arn\rport = \u0026#34;443\u0026#34;\rprotocol = \u0026#34;HTTPS\u0026#34;\rssl_policy = \u0026#34;ELBSecurityPolicy-TLS13-1-2-2021-06\u0026#34;\rcertificate_arn = var.certificate_arn\rdefault_action {\rtype = \u0026#34;forward\u0026#34;\rtarget_group_arn = aws_lb_target_group.web_app_TG.arn\r}\r} "
},
{
	"uri": "//localhost:1313/vi/6-cleanup/",
	"title": "Dọn dẹp tài nguyên  ",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ tiến hành các bước sau để xóa các tài nguyên chúng ta đã tạo trong bài thực hành này.\nXóa EC2 instance Truy cập giao diện quản trị dịch vụ EC2 Click Instances. Click chọn cả 2 instance Public Linux Instance và Private Windows Instance. Click Instance state. Click Terminate instance, sau đó click Terminate để xác nhận. Truy cập giao diện quản trị dịch vụ IAM Click Roles. Tại ô tìm kiếm , điền SSM. Click chọn SSM-Role. Click Delete, sau đó điền tên role SSM-Role và click Delete để xóa role. Click Users. Click chọn user Portfwd. Click Delete, sau đó điền tên user Portfwd và click Delete để xóa user. Xóa S3 bucket Truy cập giao diện quản trị dịch vụ System Manager - Session Manager. Click tab Preferences. Click Edit. Kéo chuột xuống dưới. Tại mục S3 logging. Bỏ chọn Enable để tắt tính năng logging. Kéo chuột xuống dưới. Click Save. Truy cập giao diện quản trị dịch vụ S3 Click chọn S3 bucket chúng ta đã tạo cho bài thực hành. ( Ví dụ : lab-fcj-bucket-0001 ) Click Empty. Điền permanently delete, sau đó click Empty để tiến hành xóa object trong bucket. Click Exit. Sau khi xóa hết object trong bucket, click Delete Điền tên S3 bucket, sau đó click Delete bucket để tiến hành xóa S3 bucket. Xóa các VPC Endpoint Truy cập vào giao diện quản trị dịch vụ VPC Click Endpoints. Chọn 4 endpoints chúng ta đã tạo cho bài thực hành bao gồm SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. Tại ô confirm , điền delete. Click Delete để tiến hành xóa các endpoints. Click biểu tượng refresh, kiểm tra tất cả các endpoints đã bị xóa trước khi làm bước tiếp theo. Xóa VPC Truy cập vào giao diện quản trị dịch vụ VPC Click Your VPCs. Click chọn Lab VPC. Click Actions. Click Delete VPC. Tại ô confirm, điền delete để xác nhận, click Delete để thực hiện xóa Lab VPC và các tài nguyên liên quan. "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]